{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 clean data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 k fold for 3 models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 gridsearchcv 3 models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 add k fold after getting best parametres from gridsearchcv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis,KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hcvdat0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "values={'0=Blood Donor':'BloodDonor', '0s=suspect Blood Donor':'suspect Blood Donor', '1=Hepatitis':'Hepatitis', '2=Fibrosis':'Fibrosis', '3=Cirrhosis':'Cirrhosis'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "##Encode target labels of cat with value between 0 and n_classes-1.\n",
    "category = le.fit_transform(data.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Encode target labels of sex  with value between 0 and n_classes-1.\n",
    "sex = le.fit_transform(data.Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex'] = sex \n",
    "data['category'] = category\n",
    "data = data.drop('Sex',axis=1)\n",
    "data = data.drop('Category',axis=1)\n",
    "##drop labels of unnaled\n",
    "data = data.drop('Unnamed: 0',axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fill null value with mean\n",
    "MeanValues0 = {\n",
    "          'ALB':data.loc[data['category'] == 0].ALB.mean() , \n",
    "          'ALP': data.loc[data['category'] == 0].ALP.mean(), \n",
    "          'ALT': data.loc[data['category'] == 0].ALT.mean(), \n",
    "          'CHOL': data.loc[data['category'] == 0].CHOL.mean(),\n",
    "          'PROT': data.loc[data['category'] == 0].PROT.mean(),           \n",
    "         }\n",
    "MeanValues1 = {\n",
    "          'ALB':data.loc[data['category'] == 1].ALB.mean() , \n",
    "          'ALP': data.loc[data['category'] == 1].ALP.mean(), \n",
    "          'ALT': data.loc[data['category'] == 1].ALT.mean(), \n",
    "          'CHOL': data.loc[data['category'] == 1].CHOL.mean(),\n",
    "          'PROT': data.loc[data['category'] == 1].PROT.mean(),           \n",
    "         }\n",
    "MeanValues2 = {\n",
    "          'ALB':data.loc[data['category'] == 2].ALB.mean() , \n",
    "          'ALP': data.loc[data['category'] == 2].ALP.mean(), \n",
    "          'ALT': data.loc[data['category'] == 2].ALT.mean(), \n",
    "          'CHOL': data.loc[data['category'] == 2].CHOL.mean(),\n",
    "          'PROT': data.loc[data['category'] == 2].PROT.mean(),           \n",
    "         }\n",
    "MeanValues3 = {\n",
    "          'ALB':data.loc[data['category'] == 3].ALB.mean() , \n",
    "          'ALP': data.loc[data['category'] == 3].ALP.mean(), \n",
    "          'ALT': data.loc[data['category'] == 3].ALT.mean(), \n",
    "          'CHOL': data.loc[data['category'] == 3].CHOL.mean(),\n",
    "          'PROT': data.loc[data['category'] == 3].PROT.mean(),           \n",
    "         }\n",
    "MeanValues4 = {\n",
    "          'ALB':data.loc[data['category'] == 4].ALB.mean() , \n",
    "          'ALP': data.loc[data['category'] == 4].ALP.mean(), \n",
    "          'ALT': data.loc[data['category'] == 4].ALT.mean(), \n",
    "          'CHOL': data.loc[data['category'] == 4].CHOL.mean(),\n",
    "          'PROT': data.loc[data['category'] == 4].PROT.mean(),           \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age          0\n",
       "ALB          1\n",
       "ALP         18\n",
       "ALT          1\n",
       "AST          0\n",
       "BIL          0\n",
       "CHE          0\n",
       "CHOL        10\n",
       "CREA         0\n",
       "GGT          0\n",
       "PROT         1\n",
       "sex          0\n",
       "category     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['category'] == 0]=data.loc[data['category'] == 0].fillna(value = MeanValues0)\n",
    "data.loc[data['category'] == 1]=data.loc[data['category'] == 1].fillna(value = MeanValues1)\n",
    "data.loc[data['category'] == 2]=data.loc[data['category'] == 2].fillna(value = MeanValues2)\n",
    "data.loc[data['category'] == 3]=data.loc[data['category'] == 3].fillna(value = MeanValues3)\n",
    "data.loc[data['category'] == 4]=data.loc[data['category'] == 4].fillna(value = MeanValues4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age         0\n",
       "ALB         0\n",
       "ALP         0\n",
       "ALT         0\n",
       "AST         0\n",
       "BIL         0\n",
       "CHE         0\n",
       "CHOL        0\n",
       "CREA        0\n",
       "GGT         0\n",
       "PROT        0\n",
       "sex         0\n",
       "category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data.iloc[:, 0:12]\n",
    "Y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "610    4\n",
       "611    4\n",
       "612    4\n",
       "613    4\n",
       "614    4\n",
       "Name: category, Length: 615, dtype: int64"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>GGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.700000</td>\n",
       "      <td>36.2</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>30.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.100000</td>\n",
       "      <td>32.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>62</td>\n",
       "      <td>32.0</td>\n",
       "      <td>416.600000</td>\n",
       "      <td>5.9</td>\n",
       "      <td>110.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.30</td>\n",
       "      <td>55.7</td>\n",
       "      <td>650.9</td>\n",
       "      <td>68.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>64</td>\n",
       "      <td>24.0</td>\n",
       "      <td>102.800000</td>\n",
       "      <td>2.9</td>\n",
       "      <td>44.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>63.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>64</td>\n",
       "      <td>29.0</td>\n",
       "      <td>87.300000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.63</td>\n",
       "      <td>66.7</td>\n",
       "      <td>64.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>46</td>\n",
       "      <td>33.0</td>\n",
       "      <td>93.220833</td>\n",
       "      <td>39.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.20</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>93.220833</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>5.30</td>\n",
       "      <td>67.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age   ALB         ALP    ALT    AST   BIL    CHE  CHOL   CREA    GGT  \\\n",
       "0     32  38.5   52.500000    7.7   22.1   7.5   6.93  3.23  106.0   12.1   \n",
       "1     32  38.5   70.300000   18.0   24.7   3.9  11.17  4.80   74.0   15.6   \n",
       "2     32  46.9   74.700000   36.2   52.6   6.1   8.84  5.20   86.0   33.2   \n",
       "3     32  43.2   52.000000   30.6   22.6  18.9   7.33  4.74   80.0   33.8   \n",
       "4     32  39.2   74.100000   32.6   24.8   9.6   9.15  4.32   76.0   29.9   \n",
       "..   ...   ...         ...    ...    ...   ...    ...   ...    ...    ...   \n",
       "610   62  32.0  416.600000    5.9  110.3  50.0   5.57  6.30   55.7  650.9   \n",
       "611   64  24.0  102.800000    2.9   44.4  20.0   1.54  3.02   63.0   35.9   \n",
       "612   64  29.0   87.300000    3.5   99.0  48.0   1.66  3.63   66.7   64.2   \n",
       "613   46  33.0   93.220833   39.0   62.0  20.0   3.56  4.20   52.0   50.0   \n",
       "614   59  36.0   93.220833  100.0   80.0  12.0   9.07  5.30   67.0   34.0   \n",
       "\n",
       "     PROT  sex  \n",
       "0    69.0    1  \n",
       "1    76.5    1  \n",
       "2    79.3    1  \n",
       "3    75.7    1  \n",
       "4    68.7    1  \n",
       "..    ...  ...  \n",
       "610  68.5    0  \n",
       "611  71.3    0  \n",
       "612  82.0    0  \n",
       "613  71.0    0  \n",
       "614  68.0    0  \n",
       "\n",
       "[615 rows x 12 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2,stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_  k-fold for 3 models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_______________________________________SVC___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        69\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.50      0.49      0.50        69\n",
      "weighted avg       1.00      0.99      0.99        69\n",
      "\n",
      "[[68  1]\n",
      " [ 0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        68\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.50      0.49      0.50        68\n",
      "weighted avg       1.00      0.99      0.99        68\n",
      "\n",
      "[[67  1]\n",
      " [ 0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.78        68\n",
      "   macro avg       0.17      0.20      0.18        68\n",
      "weighted avg       0.68      0.78      0.73        68\n",
      "\n",
      "[[53  0  0  0  1]\n",
      " [ 4  0  0  0  3]\n",
      " [ 5  0  0  2  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00      17.0\n",
      "           3       0.00      0.00      0.00      21.0\n",
      "           4       0.00      0.00      0.00      30.0\n",
      "\n",
      "    accuracy                           0.00      68.0\n",
      "   macro avg       0.00      0.00      0.00      68.0\n",
      "weighted avg       0.00      0.00      0.00      68.0\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [17  0  0  0]\n",
      " [21  0  0  0]\n",
      " [30  0  0  0]]\n",
      "0.9211330682994302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    modelsvc.fit(X_train, y_train) \n",
    "    scores.append(modelsvc.score(X_train, y_train))\n",
    "    predictions = modelsvc.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case of svm mean of accuracy is (92%) we still have the same problem of model reacal problem  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_______________________________________KNN___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        69\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97        69\n",
      "   macro avg       0.50      0.49      0.49        69\n",
      "weighted avg       1.00      0.97      0.99        69\n",
      "\n",
      "[[67  2]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        69\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.50      0.49      0.50        69\n",
      "weighted avg       1.00      0.99      0.99        69\n",
      "\n",
      "[[68  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79        68\n",
      "   macro avg       0.20      0.25      0.22        68\n",
      "weighted avg       0.64      0.79      0.71        68\n",
      "\n",
      "[[54  0  0  0]\n",
      " [ 6  0  0  1]\n",
      " [ 7  0  0  0]\n",
      " [ 0  0  0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00      17.0\n",
      "           3       0.00      0.00      0.00      21.0\n",
      "           4       0.00      0.00      0.00      30.0\n",
      "\n",
      "    accuracy                           0.00      68.0\n",
      "   macro avg       0.00      0.00      0.00      68.0\n",
      "weighted avg       0.00      0.00      0.00      68.0\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [17  0  0  0]\n",
      " [21  0  0  0]\n",
      " [30  0  0  0]]\n",
      "0.9101567063175838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"______________________________________________________________________________\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    ModelKNN.fit(X_train, y_train) \n",
    "    scores.append(ModelKNN.score(X_train, y_train))\n",
    "    predictions = ModelKNN.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case of knn mean of accuracy is (91%) we still have the same problem of model reacal problem  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "________________________________RandomForest_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        69\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.50      0.49      0.50        69\n",
      "weighted avg       1.00      0.99      0.99        69\n",
      "\n",
      "[[68  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       1.00      0.14      0.25         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79        68\n",
      "   macro avg       0.37      0.22      0.23        68\n",
      "weighted avg       0.79      0.79      0.76        68\n",
      "\n",
      "[[53  0  0  0  1]\n",
      " [ 5  0  0  0  2]\n",
      " [ 3  0  1  3  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00      17.0\n",
      "           3       0.00      0.00      0.00      21.0\n",
      "           4       0.00      0.00      0.00      30.0\n",
      "\n",
      "    accuracy                           0.00      68.0\n",
      "   macro avg       0.00      0.00      0.00      68.0\n",
      "weighted avg       0.00      0.00      0.00      68.0\n",
      "\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [17  0  0  0  0]\n",
      " [20  0  1  0  0]\n",
      " [26  4  0  0  0]]\n",
      "0.9997964997964998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"______________________________________________________________________________\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    modelrf.fit(X_train, y_train) \n",
    "    scores.append(modelrf.score(X_train, y_train))\n",
    "    predictions = modelrf.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case of rf mean of accuracy is (99%) but in all 9-fold we still have the same problem of model reacal problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_ gridsearch pour  les 3methode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "so we see that k-fold dont give us the  result that are we looking for thats why were going to do tuning parametres \n",
    "to find the best parametres with gridsearchcv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_______________________________________SVC___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('SVM', SVC())])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "steps = [('scaler', StandardScaler()), ('SVM', SVC())]\n",
    "pipeline_1 = Pipeline(steps)\n",
    "pipeline_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameteres = {'SVM__C':[0.001,0.1,10,100,10e5], 'SVM__gamma':[0.1,0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('SVM', SVC())]),\n",
       "             param_grid={'SVM__C': [0.001, 0.1, 10, 100, 1000000.0],\n",
       "                         'SVM__gamma': [0.1, 0.01]})"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(pipeline_1, param_grid=parameteres, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 100, 'SVM__gamma': 0.01}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       107\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.25      0.40      0.31         5\n",
      "           3       0.33      0.25      0.29         4\n",
      "           4       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.91       123\n",
      "   macro avg       0.46      0.49      0.47       123\n",
      "weighted avg       0.92      0.91      0.91       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[104,   0,   2,   0,   1],\n",
       "       [  1,   0,   0,   0,   0],\n",
       "       [  0,   0,   2,   2,   1],\n",
       "       [  0,   0,   3,   1,   0],\n",
       "       [  0,   0,   1,   0,   5]])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "y_true, y_pred2 = y_test, grid.predict(X_test)\n",
    "print(classification_report(y_true, y_pred2))\n",
    "cm= confusion_matrix(y_test, y_pred2)\n",
    "cm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "so as we can see the grid search give us a good accuracy  (91%) and we can see that model recall also is better\n",
    "in cas of class1 (0%) we still have q problem of recall \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_______________________________________KNN___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__algorithm': 'auto', 'knn__leaf_size': 5, 'knn__n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "pipeline_2 = Pipeline([('scaler', StandardScaler()),('nca', NeighborhoodComponentsAnalysis()), ('knn', KNeighborsClassifier())])\n",
    "pipeline_2\n",
    "#parameters\n",
    "parameteres_knn = {\n",
    "    \n",
    "    'knn__n_neighbors':[5,3,8,1],\n",
    "    'knn__algorithm':['auto','ball_tree','kd_tree','brute'],\n",
    "    'knn__leaf_size':[5,20,25,30,50]\n",
    "    \n",
    "}\n",
    "# Grdisearch\n",
    "grid = GridSearchCV(pipeline_2, param_grid=parameteres_knn, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid\n",
    "print(grid.best_params_)\n",
    "y_true, y_pred2 = y_test, grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       107\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.67      0.40      0.50         5\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       0.83      0.83      0.83         6\n",
      "\n",
      "    accuracy                           0.95       123\n",
      "   macro avg       0.64      0.60      0.61       123\n",
      "weighted avg       0.94      0.95      0.94       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[107,   0,   0,   0,   0],\n",
       "       [  1,   0,   0,   0,   0],\n",
       "       [  2,   0,   2,   0,   1],\n",
       "       [  0,   0,   1,   3,   0],\n",
       "       [  0,   0,   0,   1,   5]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred2))\n",
    "cm= confusion_matrix(y_test, y_pred2)\n",
    "cm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " as we can see knn model with the grid search give us a good accuracy (95%) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ________________________________RandomForest_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__criterion': 'entropy', 'rf__max_depth': 30, 'rf__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "pipeline_3 = Pipeline([('scaler', StandardScaler()), ('rf', rf)])\n",
    "pipeline_3\n",
    "parameteres_rf = {\n",
    "    \n",
    "    'rf__n_estimators':[20,50,80,150,200],\n",
    "    'rf__criterion':['entropy','gini'],\n",
    "    'rf__max_depth':[5,20,25,30,50,100] \n",
    "    \n",
    "}\n",
    "# Grdisearch\n",
    "grid = GridSearchCV(pipeline_3, param_grid=parameteres_rf, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       107\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.50      0.40      0.44         5\n",
      "           3       1.00      0.25      0.40         4\n",
      "           4       0.83      0.83      0.83         6\n",
      "\n",
      "    accuracy                           0.93       123\n",
      "   macro avg       0.66      0.50      0.53       123\n",
      "weighted avg       0.92      0.93      0.92       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[107,   0,   0,   0,   0],\n",
       "       [  1,   0,   0,   0,   0],\n",
       "       [  2,   0,   2,   0,   1],\n",
       "       [  1,   0,   2,   1,   0],\n",
       "       [  1,   0,   0,   0,   5]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_pred2 = y_test, grid.predict(X_test)\n",
    "print(classification_report(y_true, y_pred2))\n",
    "cm= confusion_matrix(y_test, y_pred2)\n",
    "cm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case of rf model with the grid search give us an accuracy of (93%) and  model recall still not good (0%) in class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_ apres le evalution des parametre avec  gridsearchcv on re essaye  les 3 methodes avec k-fold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "after gettin the best of parametrs of our algo we are going now to us k-fold with our algo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# _______________________________________SVC___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79        68\n",
      "   macro avg       0.20      0.25      0.22        68\n",
      "weighted avg       0.64      0.79      0.71        68\n",
      "\n",
      "[[54  0  0  0]\n",
      " [ 6  0  0  1]\n",
      " [ 7  0  0  0]\n",
      " [ 0  0  0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00      17.0\n",
      "           3       0.00      0.00      0.00      21.0\n",
      "           4       0.00      0.00      0.00      30.0\n",
      "\n",
      "    accuracy                           0.00      68.0\n",
      "   macro avg       0.00      0.00      0.00      68.0\n",
      "weighted avg       0.00      0.00      0.00      68.0\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [17  0  0  0]\n",
      " [21  0  0  0]\n",
      " [30  0  0  0]]\n",
      "0.8898286357152903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "modelsvc2 = SVC(C=10, gamma=0.01)\n",
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"______________________________________________________________________________\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    modelsvc2.fit(X_train, y_train) \n",
    "    scores.append(modelsvc2.score(X_train, y_train))\n",
    "    predictions = modelsvc2.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_______________________________________KNN___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        69\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97        69\n",
      "   macro avg       0.50      0.49      0.49        69\n",
      "weighted avg       1.00      0.97      0.99        69\n",
      "\n",
      "[[67  2]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        69\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.50      0.49      0.50        69\n",
      "weighted avg       1.00      0.99      0.99        69\n",
      "\n",
      "[[68  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        68\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.50      0.49      0.50        68\n",
      "weighted avg       1.00      0.99      0.99        68\n",
      "\n",
      "[[67  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        68\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.50      0.49      0.50        68\n",
      "weighted avg       1.00      0.99      0.99        68\n",
      "\n",
      "[[67  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        68\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.50      0.49      0.50        68\n",
      "weighted avg       1.00      0.99      0.99        68\n",
      "\n",
      "[[67  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.78        68\n",
      "   macro avg       0.20      0.25      0.22        68\n",
      "weighted avg       0.65      0.78      0.71        68\n",
      "\n",
      "[[53  0  0  1]\n",
      " [ 5  0  0  2]\n",
      " [ 7  0  0  0]\n",
      " [ 0  0  0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           2       0.33      0.06      0.10        17\n",
      "           3       0.00      0.00      0.00        21\n",
      "           4       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.01        68\n",
      "   macro avg       0.08      0.01      0.03        68\n",
      "weighted avg       0.08      0.01      0.03        68\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [16  1  0  0]\n",
      " [21  0  0  0]\n",
      " [28  2  0  0]]\n",
      "0.9264173770572308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=3,leaf_size=5)\n",
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"______________________________________________________________________________\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    knn2.fit(X_train, y_train) \n",
    "    scores.append(knn2.score(X_train, y_train))\n",
    "    predictions = knn2.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ________________________________RandomForest_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        69\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.50      0.49      0.50        69\n",
      "weighted avg       1.00      0.99      0.99        69\n",
      "\n",
      "[[68  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "[[69]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "\n",
      "    accuracy                           1.00        68\n",
      "   macro avg       1.00      1.00      1.00        68\n",
      "weighted avg       1.00      1.00      1.00        68\n",
      "\n",
      "[[68]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        68\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99        68\n",
      "   macro avg       0.50      0.49      0.50        68\n",
      "weighted avg       1.00      0.99      0.99        68\n",
      "\n",
      "[[67  1]\n",
      " [ 0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93        54\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       1.00      0.14      0.25         7\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79        68\n",
      "   macro avg       0.38      0.22      0.24        68\n",
      "weighted avg       0.80      0.79      0.76        68\n",
      "\n",
      "[[53  0  0  0  1]\n",
      " [ 4  0  0  1  2]\n",
      " [ 3  0  1  3  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "______________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00      17.0\n",
      "           3       0.00      0.00      0.00      21.0\n",
      "           4       0.00      0.00      0.00      30.0\n",
      "\n",
      "    accuracy                           0.00      68.0\n",
      "   macro avg       0.00      0.00      0.00      68.0\n",
      "weighted avg       0.00      0.00      0.00      68.0\n",
      "\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [17  0  0  0  0]\n",
      " [20  0  1  0  0]\n",
      " [27  3  0  0  0]]\n",
      "0.9985777307532335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf3 = RandomForestClassifier(max_depth=30,n_estimators=20)\n",
    "cv =KFold(n_splits=9, random_state=None, shuffle=False)\n",
    "scores=[]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"______________________________________________________________________________\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    rf3.fit(X_train, y_train) \n",
    "    scores.append(rf3.score(X_train, y_train))\n",
    "    predictions = rf3.predict(X_test) \n",
    "    cm= confusion_matrix(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(cm)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "so if we see that k-fold dont evalute our models because of dataset are extremely unbalansed \n",
    "so after our reaserch in the internet we found a python libarary call imbalanced-learn to generate a data\n",
    "set with artifical intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_ Smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.0\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - imbalanced-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py38h578d9bd_0         143 KB  conda-forge\n",
      "    imbalanced-learn-0.7.0     |             py_1          97 KB  conda-forge\n",
      "    openssl-1.1.1i             |       h7f98852_0         2.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  imbalanced-learn   conda-forge/noarch::imbalanced-learn-0.7.0-py_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2020.6.20-hecda079_0 --> 2020.12.5-ha878542_0\n",
      "  certifi                          2020.6.20-py38h924ce5b_2 --> 2020.12.5-py38h578d9bd_0\n",
      "  openssl                                 1.1.1h-h516909a_0 --> 1.1.1i-h7f98852_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "openssl-1.1.1i       | 2.1 MB    | ##################################### | 100% \n",
      "imbalanced-learn-0.7 | 97 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install imbalanced-learn -c conda-forge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "\n",
    "X,Y = oversample.fit_resample(X,Y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ________________________________SVC_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       119\n",
      "           1       0.98      1.00      0.99       128\n",
      "           2       0.91      0.89      0.90       154\n",
      "           3       0.86      0.92      0.89       131\n",
      "           4       0.99      0.96      0.98       135\n",
      "\n",
      "    accuracy                           0.94       667\n",
      "   macro avg       0.94      0.94      0.94       667\n",
      "weighted avg       0.94      0.94      0.94       667\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[110,   2,   2,   4,   1],\n",
       "       [  0, 128,   0,   0,   0],\n",
       "       [  5,   0, 137,  12,   0],\n",
       "       [  0,   0,  11, 120,   0],\n",
       "       [  0,   1,   1,   3, 130]])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsvc3 = SVC()\n",
    "modelsvc3.fit(X_train, y_train) \n",
    "predictions = modelsvc3.predict(X_test) \n",
    "print(classification_report(y_test, predictions))\n",
    "cm= confusion_matrix(y_test, predictions)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ________________________________knn_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109   3   3   1   3]\n",
      " [  0 127   0   0   1]\n",
      " [  0   0 151   3   0]\n",
      " [  0   0   0 131   0]\n",
      " [  1   3   1   0 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       119\n",
      "           1       0.95      0.99      0.97       128\n",
      "           2       0.97      0.98      0.98       154\n",
      "           3       0.97      1.00      0.98       131\n",
      "           4       0.97      0.96      0.97       135\n",
      "\n",
      "    accuracy                           0.97       667\n",
      "   macro avg       0.97      0.97      0.97       667\n",
      "weighted avg       0.97      0.97      0.97       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=5,leaf_size=5)\n",
    "knn3.fit(X_train, y_train)\n",
    "y_pred =knn3.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ________________________________RandomForest_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115   1   2   1   0]\n",
      " [  0 128   0   0   0]\n",
      " [  0   0 154   0   0]\n",
      " [  0   0   0 131   0]\n",
      " [  0   0   0   0 135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       119\n",
      "           1       0.99      1.00      1.00       128\n",
      "           2       0.99      1.00      0.99       154\n",
      "           3       0.99      1.00      1.00       131\n",
      "           4       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           0.99       667\n",
      "   macro avg       0.99      0.99      0.99       667\n",
      "weighted avg       0.99      0.99      0.99       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelrf = RandomForestClassifier(max_depth=30,n_estimators=20)\n",
    "modelrf.fit(X_train, y_train)\n",
    "y_pred = modelrf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "as we can see after using this libary our models have a good accuracy and model recall is better in 3methodes\n",
    "and the best model is RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:12]\n",
    "Y= data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatitis\n"
     ]
    }
   ],
   "source": [
    "RandomForestClassifier(max_depth=30,n_estimators=20)\n",
    "model.fit(X_train,y_train)\n",
    "df={\n",
    "    'Age':23,\n",
    "    'ALB':47,\n",
    "    'ALP':19.1,\n",
    "    'ALT':38.9,\n",
    "    'AST':164.2,\n",
    "    'BIL':17,\n",
    "    'CHE':7.09,\n",
    "    'CHOL': 3.2,\n",
    "    'CREA':79.3,\n",
    "    'GGT':90.4,\n",
    "    'PROT':70.1,\n",
    "    'sex':0\n",
    "}\n",
    "df = pd.DataFrame(data=df,columns=['Age','ALB','ALP','ALT','AST','BIL','CHE','CHOL','CREA','GGT','PROT','sex'], index=[0])\n",
    "df\n",
    "x = int(model.predict(df))\n",
    "if x == 0:\n",
    "    print(\"Blood Donor\")\n",
    "elif x == 1:\n",
    "    print(\"Suspect Blood Donor\")\n",
    "elif x == 2:\n",
    "    print(\"Hepatitis\")\n",
    "elif x == 3:\n",
    "    print(\"Fibrosis\")\n",
    "elif x == 3:\n",
    "    print(\"Cirrhosis\")\n",
    "else :\n",
    "    print(\"err\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
